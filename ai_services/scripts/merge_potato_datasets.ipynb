{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¥” Phase 2: Potato Disease Dataset Merging\n",
        "\n",
        "This notebook merges multiple potato disease datasets from Google Drive into a single unified dataset.\n",
        "\n",
        "**Features:**\n",
        "- Mounts Google Drive and reads all dataset folders\n",
        "- Identifies common disease classes across datasets\n",
        "- Creates unified `merged_potato_dataset` directory\n",
        "- Copies images with unique prefixes to avoid conflicts\n",
        "- Generates merge statistics report"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mount Google Drive\n",
        "\n",
        "Run the cell below to mount Google Drive. If you encounter errors, the code will automatically try to fix them."
      ],
      "metadata": {
        "id": "mount_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "MOUNT_PATH = '/content/drive'\n",
        "\n",
        "def mount_drive():\n",
        "    \"\"\"Mount Google Drive with automatic error handling.\"\"\"\n",
        "    \n",
        "    # Check if already mounted and working\n",
        "    if os.path.exists(os.path.join(MOUNT_PATH, 'MyDrive')):\n",
        "        print('âœ… Google Drive is already mounted!')\n",
        "        return True\n",
        "    \n",
        "    # Clear the mount point if it exists and has files\n",
        "    if os.path.exists(MOUNT_PATH):\n",
        "        print('ðŸ”„ Clearing existing mount point...')\n",
        "        try:\n",
        "            # Try to unmount first\n",
        "            drive.flush_and_unmount()\n",
        "            print('   Unmounted previous session.')\n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "        # Remove the directory if it still exists\n",
        "        if os.path.exists(MOUNT_PATH):\n",
        "            try:\n",
        "                shutil.rmtree(MOUNT_PATH)\n",
        "                print('   Cleared mount directory.')\n",
        "            except Exception as e:\n",
        "                print(f'   Warning: Could not clear directory: {e}')\n",
        "    \n",
        "    # Now mount\n",
        "    try:\n",
        "        drive.mount(MOUNT_PATH)\n",
        "        print('âœ… Google Drive mounted successfully!')\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f'âŒ Mount failed: {e}')\n",
        "        print('\\nðŸ”§ MANUAL FIX:')\n",
        "        print('   1. Go to Runtime â†’ Restart runtime')\n",
        "        print('   2. Run this cell again')\n",
        "        return False\n",
        "\n",
        "# Mount drive\n",
        "mount_drive()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "verify_mount"
      },
      "outputs": [],
      "source": [
        "# Verify Drive is mounted and check your folder structure\n",
        "drive_path = '/content/drive/MyDrive'\n",
        "\n",
        "if os.path.exists(drive_path):\n",
        "    print('âœ… Drive mounted at:', drive_path)\n",
        "    print('\\nðŸ“ Contents of MyDrive:')\n",
        "    for item in sorted(os.listdir(drive_path))[:15]:\n",
        "        print(f'   - {item}')\n",
        "else:\n",
        "    print('âŒ Drive not mounted. Please run the mount cell above first.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuration\n",
        "\n",
        "Update these paths according to your Google Drive structure."
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from pathlib import Path\n",
        "from collections import defaultdict\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "# ===== CONFIGURATION =====\n",
        "# Path to the folder containing all potato dataset folders\n",
        "DATASETS_ROOT = \"/content/drive/MyDrive/DrukFarm/data/potatoes\"\n",
        "\n",
        "# Path where the merged dataset will be created\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/DrukFarm/data/merged_potato_dataset\"\n",
        "\n",
        "# Valid image extensions\n",
        "VALID_IMAGE_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp', '.tiff', '.tif'}\n",
        "\n",
        "# Class name normalization mapping (add more as needed)\n",
        "# Maps various naming conventions to standardized names\n",
        "CLASS_NAME_MAPPING = {\n",
        "    # Early Blight variations\n",
        "    'early_blight': 'Early_Blight',\n",
        "    'early blight': 'Early_Blight',\n",
        "    'earlyblight': 'Early_Blight',\n",
        "    'early-blight': 'Early_Blight',\n",
        "    'potato___early_blight': 'Early_Blight',\n",
        "    'potato_early_blight': 'Early_Blight',\n",
        "    \n",
        "    # Late Blight variations\n",
        "    'late_blight': 'Late_Blight',\n",
        "    'late blight': 'Late_Blight',\n",
        "    'lateblight': 'Late_Blight',\n",
        "    'late-blight': 'Late_Blight',\n",
        "    'potato___late_blight': 'Late_Blight',\n",
        "    'potato_late_blight': 'Late_Blight',\n",
        "    \n",
        "    # Healthy variations\n",
        "    'healthy': 'Healthy',\n",
        "    'potato___healthy': 'Healthy',\n",
        "    'potato_healthy': 'Healthy',\n",
        "}\n",
        "\n",
        "print(\"âœ… Configuration loaded successfully!\")\n",
        "print(f\"ðŸ“ Source datasets: {DATASETS_ROOT}\")\n",
        "print(f\"ðŸ“‚ Output directory: {OUTPUT_DIR}\")\n",
        "\n",
        "# Check if source path exists\n",
        "if os.path.exists(DATASETS_ROOT):\n",
        "    print(f\"\\nâœ… Source path found!\")\n",
        "    print(f\"ðŸ“ Contents of {os.path.basename(DATASETS_ROOT)}:\")\n",
        "    for item in os.listdir(DATASETS_ROOT):\n",
        "        item_path = os.path.join(DATASETS_ROOT, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"   ðŸ“ {item}/\")\n",
        "        else:\n",
        "            print(f\"   ðŸ“„ {item}\")\n",
        "else:\n",
        "    print(f\"\\nâŒ Source path NOT found: {DATASETS_ROOT}\")\n",
        "    print(\"Please update DATASETS_ROOT to match your Drive folder structure.\")"
      ],
      "metadata": {
        "id": "configuration"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Utility Functions"
      ],
      "metadata": {
        "id": "utils_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_class_name(class_name: str) -> str:\n",
        "    \"\"\"\n",
        "    Normalize class name to a standard format.\n",
        "    Handles various naming conventions from different datasets.\n",
        "    \"\"\"\n",
        "    # Convert to lowercase for mapping lookup\n",
        "    lower_name = class_name.lower().strip()\n",
        "    \n",
        "    # Check if we have a predefined mapping\n",
        "    if lower_name in CLASS_NAME_MAPPING:\n",
        "        return CLASS_NAME_MAPPING[lower_name]\n",
        "    \n",
        "    # If not in mapping, apply general normalization\n",
        "    # Replace spaces and hyphens with underscores, capitalize each word\n",
        "    normalized = class_name.replace('-', '_').replace(' ', '_')\n",
        "    normalized = '_'.join(word.capitalize() for word in normalized.split('_'))\n",
        "    \n",
        "    return normalized\n",
        "\n",
        "\n",
        "def is_valid_image(file_path: str) -> bool:\n",
        "    \"\"\"\n",
        "    Check if a file is a valid image based on its extension.\n",
        "    \"\"\"\n",
        "    return Path(file_path).suffix.lower() in VALID_IMAGE_EXTENSIONS\n",
        "\n",
        "\n",
        "def get_unique_filename(dest_dir: str, original_name: str, dataset_prefix: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate a unique filename to avoid conflicts.\n",
        "    Format: {dataset_prefix}_{original_name}\n",
        "    If still conflicts, adds numeric suffix.\n",
        "    \"\"\"\n",
        "    stem = Path(original_name).stem\n",
        "    suffix = Path(original_name).suffix\n",
        "    \n",
        "    # Create new filename with dataset prefix\n",
        "    new_name = f\"{dataset_prefix}_{stem}{suffix}\"\n",
        "    new_path = os.path.join(dest_dir, new_name)\n",
        "    \n",
        "    # If file still exists, add numeric suffix\n",
        "    counter = 1\n",
        "    while os.path.exists(new_path):\n",
        "        new_name = f\"{dataset_prefix}_{stem}_{counter}{suffix}\"\n",
        "        new_path = os.path.join(dest_dir, new_name)\n",
        "        counter += 1\n",
        "    \n",
        "    return new_name\n",
        "\n",
        "\n",
        "def sanitize_prefix(name: str) -> str:\n",
        "    \"\"\"\n",
        "    Sanitize dataset name to create a valid file prefix.\n",
        "    \"\"\"\n",
        "    # Replace invalid characters with underscore\n",
        "    sanitized = ''.join(c if c.isalnum() or c in '-_' else '_' for c in name)\n",
        "    return sanitized[:20]  # Limit length\n",
        "\n",
        "\n",
        "print(\"âœ… Utility functions defined!\")"
      ],
      "metadata": {
        "id": "utility_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Dataset Discovery"
      ],
      "metadata": {
        "id": "discovery_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def discover_datasets(root_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Discover all datasets and their classes in the root directory.\n",
        "    Returns a dictionary mapping dataset names to their class folders.\n",
        "    \"\"\"\n",
        "    datasets = {}\n",
        "    \n",
        "    if not os.path.exists(root_path):\n",
        "        print(f\"âŒ Error: Root path does not exist: {root_path}\")\n",
        "        return datasets\n",
        "    \n",
        "    # Iterate through items in the root directory\n",
        "    for item in os.listdir(root_path):\n",
        "        item_path = os.path.join(root_path, item)\n",
        "        \n",
        "        # Skip if not a directory\n",
        "        if not os.path.isdir(item_path):\n",
        "            print(f\"â­ï¸ Skipping non-directory: {item}\")\n",
        "            continue\n",
        "        \n",
        "        # Check if this directory contains class subdirectories\n",
        "        classes = {}\n",
        "        for class_item in os.listdir(item_path):\n",
        "            class_path = os.path.join(item_path, class_item)\n",
        "            \n",
        "            if os.path.isdir(class_path):\n",
        "                # Count images in this class directory\n",
        "                image_count = sum(\n",
        "                    1 for f in os.listdir(class_path)\n",
        "                    if is_valid_image(os.path.join(class_path, f))\n",
        "                )\n",
        "                if image_count > 0:\n",
        "                    normalized_class = normalize_class_name(class_item)\n",
        "                    classes[class_item] = {\n",
        "                        'path': class_path,\n",
        "                        'normalized_name': normalized_class,\n",
        "                        'image_count': image_count\n",
        "                    }\n",
        "        \n",
        "        if classes:\n",
        "            datasets[item] = classes\n",
        "            print(f\"ðŸ“ Found dataset: {item} with {len(classes)} classes\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Skipping folder (no valid classes): {item}\")\n",
        "    \n",
        "    return datasets\n",
        "\n",
        "\n",
        "# Discover datasets\n",
        "print(\"ðŸ” Discovering datasets...\\n\")\n",
        "discovered_datasets = discover_datasets(DATASETS_ROOT)\n",
        "\n",
        "print(f\"\\nâœ… Discovered {len(discovered_datasets)} dataset(s)\")"
      ],
      "metadata": {
        "id": "discover_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Analyze Classes Across Datasets"
      ],
      "metadata": {
        "id": "analyze_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_classes(datasets: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Analyze and aggregate class information across all datasets.\n",
        "    \"\"\"\n",
        "    class_summary = defaultdict(lambda: {'datasets': [], 'total_images': 0})\n",
        "    \n",
        "    for dataset_name, classes in datasets.items():\n",
        "        for original_class, info in classes.items():\n",
        "            normalized = info['normalized_name']\n",
        "            class_summary[normalized]['datasets'].append({\n",
        "                'dataset': dataset_name,\n",
        "                'original_name': original_class,\n",
        "                'path': info['path'],\n",
        "                'image_count': info['image_count']\n",
        "            })\n",
        "            class_summary[normalized]['total_images'] += info['image_count']\n",
        "    \n",
        "    return dict(class_summary)\n",
        "\n",
        "\n",
        "# Analyze classes\n",
        "class_analysis = analyze_classes(discovered_datasets)\n",
        "\n",
        "print(\"ðŸ“Š CLASS ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 50)\n",
        "for class_name, info in sorted(class_analysis.items()):\n",
        "    print(f\"\\nðŸ·ï¸ {class_name}:\")\n",
        "    print(f\"   Total images: {info['total_images']}\")\n",
        "    print(f\"   Found in {len(info['datasets'])} dataset(s):\")\n",
        "    for ds in info['datasets']:\n",
        "        print(f\"      - {ds['dataset']}: {ds['image_count']} images (as '{ds['original_name']}')\") "
      ],
      "metadata": {
        "id": "analyze_classes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Merge Datasets"
      ],
      "metadata": {
        "id": "merge_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_datasets(class_analysis: dict, output_dir: str) -> dict:\n",
        "    \"\"\"\n",
        "    Merge all datasets into a unified directory structure.\n",
        "    Returns statistics about the merge operation.\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'classes': {},\n",
        "        'total_images_copied': 0,\n",
        "        'skipped_files': 0,\n",
        "        'errors': []\n",
        "    }\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    print(f\"ðŸ“‚ Created output directory: {output_dir}\\n\")\n",
        "    \n",
        "    # Process each normalized class\n",
        "    for class_name, info in sorted(class_analysis.items()):\n",
        "        print(f\"\\nðŸ”„ Processing class: {class_name}\")\n",
        "        \n",
        "        # Create class directory\n",
        "        class_dir = os.path.join(output_dir, class_name)\n",
        "        os.makedirs(class_dir, exist_ok=True)\n",
        "        \n",
        "        class_stats = {\n",
        "            'images_copied': 0,\n",
        "            'sources': {}\n",
        "        }\n",
        "        \n",
        "        # Copy images from each dataset\n",
        "        for dataset_info in info['datasets']:\n",
        "            dataset_name = dataset_info['dataset']\n",
        "            source_path = dataset_info['path']\n",
        "            prefix = sanitize_prefix(dataset_name)\n",
        "            \n",
        "            copied_count = 0\n",
        "            \n",
        "            # Copy each image\n",
        "            for filename in os.listdir(source_path):\n",
        "                source_file = os.path.join(source_path, filename)\n",
        "                \n",
        "                # Skip non-image files\n",
        "                if not is_valid_image(source_file):\n",
        "                    stats['skipped_files'] += 1\n",
        "                    continue\n",
        "                \n",
        "                # Skip directories\n",
        "                if os.path.isdir(source_file):\n",
        "                    continue\n",
        "                \n",
        "                try:\n",
        "                    # Generate unique filename\n",
        "                    new_filename = get_unique_filename(class_dir, filename, prefix)\n",
        "                    dest_file = os.path.join(class_dir, new_filename)\n",
        "                    \n",
        "                    # Copy file\n",
        "                    shutil.copy2(source_file, dest_file)\n",
        "                    copied_count += 1\n",
        "                    stats['total_images_copied'] += 1\n",
        "                    \n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Error copying {source_file}: {str(e)}\"\n",
        "                    stats['errors'].append(error_msg)\n",
        "                    print(f\"   âŒ {error_msg}\")\n",
        "            \n",
        "            class_stats['sources'][dataset_name] = copied_count\n",
        "            class_stats['images_copied'] += copied_count\n",
        "            print(f\"   âœ… Copied {copied_count} images from '{dataset_name}'\")\n",
        "        \n",
        "        stats['classes'][class_name] = class_stats\n",
        "        print(f\"   ðŸ“Š Total for {class_name}: {class_stats['images_copied']} images\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "\n",
        "# Execute merge\n",
        "print(\"ðŸš€ STARTING DATASET MERGE\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "merge_stats = merge_datasets(class_analysis, OUTPUT_DIR)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"âœ… MERGE COMPLETED!\")"
      ],
      "metadata": {
        "id": "merge_datasets"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Generate Merge Report"
      ],
      "metadata": {
        "id": "report_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_report(stats: dict, output_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Generate and display a comprehensive merge report.\n",
        "    Also saves the report to a JSON file.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ðŸ“‹ MERGE REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nðŸ“… Merge Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "    print(f\"ðŸ“‚ Output Directory: {output_dir}\")\n",
        "    print(f\"\\nðŸ“Š SUMMARY:\")\n",
        "    print(f\"   â€¢ Total images copied: {stats['total_images_copied']}\")\n",
        "    print(f\"   â€¢ Total classes created: {len(stats['classes'])}\")\n",
        "    print(f\"   â€¢ Non-image files skipped: {stats['skipped_files']}\")\n",
        "    print(f\"   â€¢ Errors encountered: {len(stats['errors'])}\")\n",
        "    \n",
        "    print(f\"\\nðŸ“ CLASS BREAKDOWN:\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for class_name, class_stats in sorted(stats['classes'].items()):\n",
        "        print(f\"\\n   ðŸ·ï¸ {class_name}: {class_stats['images_copied']} images\")\n",
        "        for source, count in class_stats['sources'].items():\n",
        "            print(f\"      â””â”€ {source}: {count}\")\n",
        "    \n",
        "    if stats['errors']:\n",
        "        print(f\"\\nâš ï¸ ERRORS:\")\n",
        "        for error in stats['errors'][:10]:  # Show first 10 errors\n",
        "            print(f\"   â€¢ {error}\")\n",
        "        if len(stats['errors']) > 10:\n",
        "            print(f\"   ... and {len(stats['errors']) - 10} more errors\")\n",
        "    \n",
        "    # Save report to JSON\n",
        "    report_path = os.path.join(output_dir, \"merge_report.json\")\n",
        "    report_data = {\n",
        "        'merge_date': datetime.now().isoformat(),\n",
        "        'output_directory': output_dir,\n",
        "        'total_images_copied': stats['total_images_copied'],\n",
        "        'total_classes': len(stats['classes']),\n",
        "        'skipped_files': stats['skipped_files'],\n",
        "        'errors_count': len(stats['errors']),\n",
        "        'class_details': stats['classes'],\n",
        "        'errors': stats['errors']\n",
        "    }\n",
        "    \n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(report_data, f, indent=2)\n",
        "    \n",
        "    print(f\"\\nðŸ’¾ Report saved to: {report_path}\")\n",
        "\n",
        "\n",
        "# Generate report\n",
        "generate_report(merge_stats, OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "generate_report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Verify Merged Dataset"
      ],
      "metadata": {
        "id": "verify_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_merged_dataset(output_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Verify the merged dataset structure and contents.\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ðŸ” VERIFICATION\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    if not os.path.exists(output_dir):\n",
        "        print(\"âŒ Output directory does not exist!\")\n",
        "        return\n",
        "    \n",
        "    print(f\"\\nðŸ“‚ Merged Dataset Structure:\")\n",
        "    print(f\"{os.path.basename(output_dir)}/\")\n",
        "    \n",
        "    total_images = 0\n",
        "    \n",
        "    for class_folder in sorted(os.listdir(output_dir)):\n",
        "        class_path = os.path.join(output_dir, class_folder)\n",
        "        \n",
        "        if os.path.isdir(class_path):\n",
        "            image_count = len([\n",
        "                f for f in os.listdir(class_path)\n",
        "                if is_valid_image(os.path.join(class_path, f))\n",
        "            ])\n",
        "            total_images += image_count\n",
        "            print(f\"â”œâ”€â”€ {class_folder}/  ({image_count} images)\")\n",
        "    \n",
        "    print(f\"\\nâœ… Total verified images: {total_images}\")\n",
        "    print(\"\\nðŸŽ‰ Dataset is ready for preprocessing and model training!\")\n",
        "\n",
        "\n",
        "# Verify\n",
        "verify_merged_dataset(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "verify_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Sample Images Preview (Optional)"
      ],
      "metadata": {
        "id": "preview_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "def preview_samples(output_dir: str, samples_per_class: int = 3) -> None:\n",
        "    \"\"\"\n",
        "    Display sample images from each class in the merged dataset.\n",
        "    \"\"\"\n",
        "    classes = sorted([\n",
        "        d for d in os.listdir(output_dir)\n",
        "        if os.path.isdir(os.path.join(output_dir, d))\n",
        "    ])\n",
        "    \n",
        "    if not classes:\n",
        "        print(\"No classes found!\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(len(classes), samples_per_class, figsize=(12, 4 * len(classes)))\n",
        "    \n",
        "    if len(classes) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(output_dir, class_name)\n",
        "        images = [\n",
        "            f for f in os.listdir(class_path)\n",
        "            if is_valid_image(os.path.join(class_path, f))\n",
        "        ]\n",
        "        \n",
        "        # Sample random images\n",
        "        sample_images = random.sample(images, min(samples_per_class, len(images)))\n",
        "        \n",
        "        for j, img_name in enumerate(sample_images):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            img = Image.open(img_path)\n",
        "            \n",
        "            ax = axes[i][j] if len(classes) > 1 else axes[j]\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"{class_name}\\n{img_name[:30]}...\", fontsize=8)\n",
        "            ax.axis('off')\n",
        "        \n",
        "        # Fill remaining slots with blank\n",
        "        for j in range(len(sample_images), samples_per_class):\n",
        "            ax = axes[i][j] if len(classes) > 1 else axes[j]\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(\"Sample Images from Merged Dataset\", fontsize=14, y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Preview samples\n",
        "preview_samples(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "preview_samples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## âœ… Phase 2 Complete!\n",
        "\n",
        "The merged dataset has been created at:\n",
        "```\n",
        "/content/drive/MyDrive/DrukFarm/data/merged_potato_dataset/\n",
        "â”œâ”€â”€ Early_Blight/\n",
        "â”œâ”€â”€ Late_Blight/\n",
        "â””â”€â”€ Healthy/\n",
        "```\n",
        "\n",
        "**Next Steps:**\n",
        "- Phase 3: Data Preprocessing (resizing, augmentation)\n",
        "- Phase 4: Model Training"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
