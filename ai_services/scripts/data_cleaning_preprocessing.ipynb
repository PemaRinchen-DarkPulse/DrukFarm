{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ðŸ¥” Phase 3: Data Cleaning & Preprocessing\n",
        "\n",
        "This notebook validates, cleans, and preprocesses the merged potato disease dataset for model training.\n",
        "\n",
        "**Features:**\n",
        "- Validates all images and identifies corrupted/unreadable files\n",
        "- Removes or isolates invalid images\n",
        "- Standardizes images to consistent format (size, color, normalization)\n",
        "- Generates detailed cleaning report\n",
        "- Maintains class-wise folder structure"
      ],
      "metadata": {
        "id": "intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Mount Google Drive"
      ],
      "metadata": {
        "id": "mount_header"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "    print('âœ… Google Drive mounted successfully!')\n",
        "except Exception as e:\n",
        "    print(f'âŒ Error: {e}')\n",
        "    print('\\nðŸ”§ Try: Sign out of all accounts, use regular browser, enable cookies')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Install Dependencies & Configuration"
      ],
      "metadata": {
        "id": "config_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages (if not already installed)\n",
        "!pip install -q pillow opencv-python-headless tqdm\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ… Dependencies loaded!\")"
      ],
      "metadata": {
        "id": "install_deps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===== CONFIGURATION =====\n",
        "\n",
        "# Source: Merged dataset from Phase 2\n",
        "SOURCE_DIR = \"/content/drive/MyDrive/DrukFarm/data/merged_potato_dataset\"\n",
        "\n",
        "# Output: Cleaned and preprocessed dataset\n",
        "OUTPUT_DIR = \"/content/drive/MyDrive/DrukFarm/data/preprocessed_potato_dataset\"\n",
        "\n",
        "# Directory to store corrupted/invalid images (optional backup)\n",
        "QUARANTINE_DIR = \"/content/drive/MyDrive/DrukFarm/data/quarantine_images\"\n",
        "\n",
        "# Preprocessing parameters\n",
        "TARGET_SIZE = (224, 224)  # Standard CNN input size (e.g., ResNet, VGG, EfficientNet)\n",
        "COLOR_MODE = \"RGB\"        # Ensure consistent color format\n",
        "IMAGE_FORMAT = \"jpg\"      # Output format\n",
        "JPEG_QUALITY = 95         # Quality for saved images\n",
        "\n",
        "# Valid image extensions\n",
        "VALID_EXTENSIONS = {'.jpg', '.jpeg', '.png', '.bmp', '.gif', '.webp', '.tiff', '.tif'}\n",
        "\n",
        "# Whether to quarantine invalid images or delete them\n",
        "QUARANTINE_INVALID = True  # Set to False to delete instead\n",
        "\n",
        "print(\"âœ… Configuration loaded!\")\n",
        "print(f\"ðŸ“ Source: {SOURCE_DIR}\")\n",
        "print(f\"ðŸ“‚ Output: {OUTPUT_DIR}\")\n",
        "print(f\"ðŸ“ Target size: {TARGET_SIZE}\")\n",
        "print(f\"ðŸŽ¨ Color mode: {COLOR_MODE}\")"
      ],
      "metadata": {
        "id": "configuration"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Image Validation Functions"
      ],
      "metadata": {
        "id": "validation_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_valid_extension(filepath: str) -> bool:\n",
        "    \"\"\"Check if file has a valid image extension.\"\"\"\n",
        "    return Path(filepath).suffix.lower() in VALID_EXTENSIONS\n",
        "\n",
        "\n",
        "def validate_image(filepath: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Validate an image file.\n",
        "    Returns: (is_valid, error_message, image_info)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Check file exists\n",
        "        if not os.path.exists(filepath):\n",
        "            return False, \"File not found\", None\n",
        "        \n",
        "        # Check file size (skip empty files)\n",
        "        if os.path.getsize(filepath) == 0:\n",
        "            return False, \"Empty file (0 bytes)\", None\n",
        "        \n",
        "        # Try opening with PIL\n",
        "        with Image.open(filepath) as img:\n",
        "            # Verify the image can be loaded\n",
        "            img.verify()\n",
        "        \n",
        "        # Re-open to get actual data (verify closes the file)\n",
        "        with Image.open(filepath) as img:\n",
        "            # Force load pixel data to catch truncated images\n",
        "            img.load()\n",
        "            \n",
        "            # Get image info\n",
        "            info = {\n",
        "                'width': img.width,\n",
        "                'height': img.height,\n",
        "                'mode': img.mode,\n",
        "                'format': img.format\n",
        "            }\n",
        "            \n",
        "            # Check minimum dimensions\n",
        "            if img.width < 10 or img.height < 10:\n",
        "                return False, f\"Image too small ({img.width}x{img.height})\", info\n",
        "            \n",
        "            return True, None, info\n",
        "            \n",
        "    except Exception as e:\n",
        "        return False, str(e), None\n",
        "\n",
        "\n",
        "def validate_with_opencv(filepath: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Secondary validation using OpenCV.\n",
        "    Returns: (is_valid, error_message)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = cv2.imread(filepath)\n",
        "        if img is None:\n",
        "            return False, \"OpenCV could not read image\"\n",
        "        if img.size == 0:\n",
        "            return False, \"OpenCV read empty image\"\n",
        "        return True, None\n",
        "    except Exception as e:\n",
        "        return False, str(e)\n",
        "\n",
        "\n",
        "print(\"âœ… Validation functions defined!\")"
      ],
      "metadata": {
        "id": "validation_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Preprocessing Functions"
      ],
      "metadata": {
        "id": "preprocessing_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_image(filepath: str, output_path: str) -> tuple:\n",
        "    \"\"\"\n",
        "    Preprocess a single image:\n",
        "    - Resize to target dimensions\n",
        "    - Convert to RGB\n",
        "    - Save in consistent format\n",
        "    \n",
        "    Returns: (success, error_message)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Load image\n",
        "        with Image.open(filepath) as img:\n",
        "            # Convert to RGB (handles RGBA, grayscale, palette images)\n",
        "            if img.mode != 'RGB':\n",
        "                # Handle transparency by pasting on white background\n",
        "                if img.mode == 'RGBA':\n",
        "                    background = Image.new('RGB', img.size, (255, 255, 255))\n",
        "                    background.paste(img, mask=img.split()[3])\n",
        "                    img = background\n",
        "                else:\n",
        "                    img = img.convert('RGB')\n",
        "            \n",
        "            # Resize using high-quality resampling\n",
        "            img_resized = img.resize(TARGET_SIZE, Image.Resampling.LANCZOS)\n",
        "            \n",
        "            # Save with consistent format and quality\n",
        "            img_resized.save(output_path, 'JPEG', quality=JPEG_QUALITY)\n",
        "            \n",
        "            return True, None\n",
        "            \n",
        "    except Exception as e:\n",
        "        return False, str(e)\n",
        "\n",
        "\n",
        "def get_image_stats(image_path: str) -> dict:\n",
        "    \"\"\"\n",
        "    Get basic statistics about an image.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        img = np.array(Image.open(image_path))\n",
        "        return {\n",
        "            'mean': img.mean(),\n",
        "            'std': img.std(),\n",
        "            'min': img.min(),\n",
        "            'max': img.max()\n",
        "        }\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "\n",
        "print(\"âœ… Preprocessing functions defined!\")"
      ],
      "metadata": {
        "id": "preprocessing_functions"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Scan & Validate Dataset"
      ],
      "metadata": {
        "id": "scan_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scan_and_validate_dataset(source_dir: str) -> dict:\n",
        "    \"\"\"\n",
        "    Scan the entire dataset and validate all images.\n",
        "    Returns detailed report of valid and invalid files.\n",
        "    \"\"\"\n",
        "    report = {\n",
        "        'scan_date': datetime.now().isoformat(),\n",
        "        'source_directory': source_dir,\n",
        "        'classes': {},\n",
        "        'summary': {\n",
        "            'total_files': 0,\n",
        "            'valid_images': 0,\n",
        "            'invalid_images': 0,\n",
        "            'non_image_files': 0\n",
        "        },\n",
        "        'invalid_files': []\n",
        "    }\n",
        "    \n",
        "    if not os.path.exists(source_dir):\n",
        "        print(f\"âŒ Source directory not found: {source_dir}\")\n",
        "        return report\n",
        "    \n",
        "    # Get all class directories\n",
        "    class_dirs = [d for d in os.listdir(source_dir) \n",
        "                  if os.path.isdir(os.path.join(source_dir, d))]\n",
        "    \n",
        "    print(f\"ðŸ“ Found {len(class_dirs)} classes to validate\\n\")\n",
        "    \n",
        "    for class_name in sorted(class_dirs):\n",
        "        class_path = os.path.join(source_dir, class_name)\n",
        "        \n",
        "        class_report = {\n",
        "            'valid': [],\n",
        "            'invalid': [],\n",
        "            'non_image': [],\n",
        "            'stats': {\n",
        "                'total': 0,\n",
        "                'valid': 0,\n",
        "                'invalid': 0,\n",
        "                'non_image': 0\n",
        "            }\n",
        "        }\n",
        "        \n",
        "        files = os.listdir(class_path)\n",
        "        print(f\"ðŸ” Validating {class_name}: {len(files)} files...\")\n",
        "        \n",
        "        for filename in tqdm(files, desc=f\"  {class_name}\", leave=False):\n",
        "            filepath = os.path.join(class_path, filename)\n",
        "            \n",
        "            # Skip directories\n",
        "            if os.path.isdir(filepath):\n",
        "                continue\n",
        "            \n",
        "            class_report['stats']['total'] += 1\n",
        "            report['summary']['total_files'] += 1\n",
        "            \n",
        "            # Check extension\n",
        "            if not is_valid_extension(filepath):\n",
        "                class_report['non_image'].append(filename)\n",
        "                class_report['stats']['non_image'] += 1\n",
        "                report['summary']['non_image_files'] += 1\n",
        "                continue\n",
        "            \n",
        "            # Validate image\n",
        "            is_valid, error, info = validate_image(filepath)\n",
        "            \n",
        "            if is_valid:\n",
        "                # Double-check with OpenCV\n",
        "                cv_valid, cv_error = validate_with_opencv(filepath)\n",
        "                if cv_valid:\n",
        "                    class_report['valid'].append(filename)\n",
        "                    class_report['stats']['valid'] += 1\n",
        "                    report['summary']['valid_images'] += 1\n",
        "                else:\n",
        "                    class_report['invalid'].append({\n",
        "                        'filename': filename,\n",
        "                        'error': f\"OpenCV: {cv_error}\"\n",
        "                    })\n",
        "                    class_report['stats']['invalid'] += 1\n",
        "                    report['summary']['invalid_images'] += 1\n",
        "                    report['invalid_files'].append({\n",
        "                        'class': class_name,\n",
        "                        'filename': filename,\n",
        "                        'path': filepath,\n",
        "                        'error': f\"OpenCV: {cv_error}\"\n",
        "                    })\n",
        "            else:\n",
        "                class_report['invalid'].append({\n",
        "                    'filename': filename,\n",
        "                    'error': error\n",
        "                })\n",
        "                class_report['stats']['invalid'] += 1\n",
        "                report['summary']['invalid_images'] += 1\n",
        "                report['invalid_files'].append({\n",
        "                    'class': class_name,\n",
        "                    'filename': filename,\n",
        "                    'path': filepath,\n",
        "                    'error': error\n",
        "                })\n",
        "        \n",
        "        report['classes'][class_name] = class_report\n",
        "        print(f\"   âœ… Valid: {class_report['stats']['valid']} | âŒ Invalid: {class_report['stats']['invalid']} | â­ï¸ Non-image: {class_report['stats']['non_image']}\")\n",
        "    \n",
        "    return report\n",
        "\n",
        "\n",
        "# Run validation\n",
        "print(\"ðŸ” DATASET VALIDATION\")\n",
        "print(\"=\" * 50)\n",
        "validation_report = scan_and_validate_dataset(SOURCE_DIR)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 50)\n",
        "print(\"ðŸ“Š VALIDATION SUMMARY\")\n",
        "print(f\"   Total files scanned: {validation_report['summary']['total_files']}\")\n",
        "print(f\"   âœ… Valid images: {validation_report['summary']['valid_images']}\")\n",
        "print(f\"   âŒ Invalid images: {validation_report['summary']['invalid_images']}\")\n",
        "print(f\"   â­ï¸ Non-image files: {validation_report['summary']['non_image_files']}\")"
      ],
      "metadata": {
        "id": "scan_validate"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Handle Invalid Images"
      ],
      "metadata": {
        "id": "handle_invalid_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def handle_invalid_images(validation_report: dict, quarantine: bool = True) -> dict:\n",
        "    \"\"\"\n",
        "    Handle invalid images by quarantining or deleting them.\n",
        "    \"\"\"\n",
        "    results = {\n",
        "        'quarantined': [],\n",
        "        'deleted': [],\n",
        "        'errors': []\n",
        "    }\n",
        "    \n",
        "    invalid_files = validation_report.get('invalid_files', [])\n",
        "    \n",
        "    if not invalid_files:\n",
        "        print(\"âœ… No invalid images to handle!\")\n",
        "        return results\n",
        "    \n",
        "    print(f\"\\nðŸ”§ Processing {len(invalid_files)} invalid files...\")\n",
        "    \n",
        "    if quarantine:\n",
        "        # Create quarantine directory\n",
        "        os.makedirs(QUARANTINE_DIR, exist_ok=True)\n",
        "    \n",
        "    for invalid in tqdm(invalid_files, desc=\"Processing invalid files\"):\n",
        "        filepath = invalid['path']\n",
        "        class_name = invalid['class']\n",
        "        filename = invalid['filename']\n",
        "        \n",
        "        try:\n",
        "            if quarantine:\n",
        "                # Create class subdirectory in quarantine\n",
        "                quarantine_class_dir = os.path.join(QUARANTINE_DIR, class_name)\n",
        "                os.makedirs(quarantine_class_dir, exist_ok=True)\n",
        "                \n",
        "                # Move file to quarantine\n",
        "                dest_path = os.path.join(quarantine_class_dir, filename)\n",
        "                shutil.move(filepath, dest_path)\n",
        "                results['quarantined'].append({\n",
        "                    'original': filepath,\n",
        "                    'quarantine': dest_path\n",
        "                })\n",
        "            else:\n",
        "                # Delete the file\n",
        "                os.remove(filepath)\n",
        "                results['deleted'].append(filepath)\n",
        "                \n",
        "        except Exception as e:\n",
        "            results['errors'].append({\n",
        "                'file': filepath,\n",
        "                'error': str(e)\n",
        "            })\n",
        "    \n",
        "    print(f\"\\nðŸ“Š Results:\")\n",
        "    if quarantine:\n",
        "        print(f\"   ðŸ“¦ Quarantined: {len(results['quarantined'])} files\")\n",
        "        print(f\"   ðŸ“ Quarantine location: {QUARANTINE_DIR}\")\n",
        "    else:\n",
        "        print(f\"   ðŸ—‘ï¸ Deleted: {len(results['deleted'])} files\")\n",
        "    \n",
        "    if results['errors']:\n",
        "        print(f\"   âŒ Errors: {len(results['errors'])}\")\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "# Handle invalid images\n",
        "if validation_report['summary']['invalid_images'] > 0:\n",
        "    print(\"\\nðŸ”§ HANDLING INVALID IMAGES\")\n",
        "    print(\"=\" * 50)\n",
        "    cleanup_results = handle_invalid_images(validation_report, quarantine=QUARANTINE_INVALID)\n",
        "else:\n",
        "    print(\"\\nâœ… No invalid images to handle!\")\n",
        "    cleanup_results = {'quarantined': [], 'deleted': [], 'errors': []}"
      ],
      "metadata": {
        "id": "handle_invalid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Preprocess Valid Images"
      ],
      "metadata": {
        "id": "preprocess_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataset(source_dir: str, output_dir: str, validation_report: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Preprocess all valid images in the dataset.\n",
        "    \"\"\"\n",
        "    stats = {\n",
        "        'processed': 0,\n",
        "        'failed': 0,\n",
        "        'classes': {},\n",
        "        'errors': []\n",
        "    }\n",
        "    \n",
        "    # Create output directory\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    print(f\"ðŸ“‚ Output directory: {output_dir}\\n\")\n",
        "    \n",
        "    for class_name, class_data in validation_report['classes'].items():\n",
        "        valid_files = class_data['valid']\n",
        "        \n",
        "        if not valid_files:\n",
        "            print(f\"âš ï¸ {class_name}: No valid images to process\")\n",
        "            continue\n",
        "        \n",
        "        # Create class output directory\n",
        "        class_output_dir = os.path.join(output_dir, class_name)\n",
        "        os.makedirs(class_output_dir, exist_ok=True)\n",
        "        \n",
        "        class_processed = 0\n",
        "        class_failed = 0\n",
        "        \n",
        "        print(f\"ðŸ”„ Processing {class_name}: {len(valid_files)} images...\")\n",
        "        \n",
        "        for filename in tqdm(valid_files, desc=f\"  {class_name}\", leave=False):\n",
        "            source_path = os.path.join(source_dir, class_name, filename)\n",
        "            \n",
        "            # Generate new filename with consistent extension\n",
        "            new_filename = Path(filename).stem + f\".{IMAGE_FORMAT}\"\n",
        "            output_path = os.path.join(class_output_dir, new_filename)\n",
        "            \n",
        "            # Handle potential filename conflicts\n",
        "            counter = 1\n",
        "            while os.path.exists(output_path):\n",
        "                new_filename = f\"{Path(filename).stem}_{counter}.{IMAGE_FORMAT}\"\n",
        "                output_path = os.path.join(class_output_dir, new_filename)\n",
        "                counter += 1\n",
        "            \n",
        "            # Preprocess image\n",
        "            success, error = preprocess_image(source_path, output_path)\n",
        "            \n",
        "            if success:\n",
        "                class_processed += 1\n",
        "                stats['processed'] += 1\n",
        "            else:\n",
        "                class_failed += 1\n",
        "                stats['failed'] += 1\n",
        "                stats['errors'].append({\n",
        "                    'class': class_name,\n",
        "                    'file': filename,\n",
        "                    'error': error\n",
        "                })\n",
        "        \n",
        "        stats['classes'][class_name] = {\n",
        "            'processed': class_processed,\n",
        "            'failed': class_failed\n",
        "        }\n",
        "        print(f\"   âœ… Processed: {class_processed} | âŒ Failed: {class_failed}\")\n",
        "    \n",
        "    return stats\n",
        "\n",
        "\n",
        "# Preprocess dataset\n",
        "print(\"\\nðŸ”„ PREPROCESSING DATASET\")\n",
        "print(\"=\" * 50)\n",
        "print(f\"ðŸ“ Target size: {TARGET_SIZE}\")\n",
        "print(f\"ðŸŽ¨ Color mode: {COLOR_MODE}\")\n",
        "print(f\"ðŸ“„ Output format: {IMAGE_FORMAT}\")\n",
        "print()\n",
        "\n",
        "preprocessing_stats = preprocess_dataset(SOURCE_DIR, OUTPUT_DIR, validation_report)"
      ],
      "metadata": {
        "id": "preprocess_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Generate Final Report"
      ],
      "metadata": {
        "id": "report_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_final_report(validation_report: dict, preprocessing_stats: dict, \n",
        "                          cleanup_results: dict, output_dir: str) -> dict:\n",
        "    \"\"\"\n",
        "    Generate comprehensive final report.\n",
        "    \"\"\"\n",
        "    report = {\n",
        "        'phase': 'Phase 3: Data Cleaning & Preprocessing',\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'configuration': {\n",
        "            'target_size': TARGET_SIZE,\n",
        "            'color_mode': COLOR_MODE,\n",
        "            'image_format': IMAGE_FORMAT,\n",
        "            'source_dir': SOURCE_DIR,\n",
        "            'output_dir': OUTPUT_DIR\n",
        "        },\n",
        "        'validation': {\n",
        "            'total_files_scanned': validation_report['summary']['total_files'],\n",
        "            'valid_images': validation_report['summary']['valid_images'],\n",
        "            'invalid_images': validation_report['summary']['invalid_images'],\n",
        "            'non_image_files': validation_report['summary']['non_image_files']\n",
        "        },\n",
        "        'cleanup': {\n",
        "            'quarantined': len(cleanup_results.get('quarantined', [])),\n",
        "            'deleted': len(cleanup_results.get('deleted', [])),\n",
        "            'errors': len(cleanup_results.get('errors', []))\n",
        "        },\n",
        "        'preprocessing': {\n",
        "            'total_processed': preprocessing_stats['processed'],\n",
        "            'total_failed': preprocessing_stats['failed'],\n",
        "            'class_breakdown': preprocessing_stats['classes']\n",
        "        },\n",
        "        'output_dataset': {\n",
        "            'location': output_dir,\n",
        "            'classes': {}\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    # Count final output\n",
        "    if os.path.exists(output_dir):\n",
        "        for class_name in os.listdir(output_dir):\n",
        "            class_path = os.path.join(output_dir, class_name)\n",
        "            if os.path.isdir(class_path):\n",
        "                count = len([f for f in os.listdir(class_path) if f.endswith(f'.{IMAGE_FORMAT}')])\n",
        "                report['output_dataset']['classes'][class_name] = count\n",
        "    \n",
        "    report['output_dataset']['total_images'] = sum(report['output_dataset']['classes'].values())\n",
        "    \n",
        "    # Save report\n",
        "    report_path = os.path.join(output_dir, 'preprocessing_report.json')\n",
        "    with open(report_path, 'w') as f:\n",
        "        json.dump(report, f, indent=2)\n",
        "    \n",
        "    return report\n",
        "\n",
        "\n",
        "# Generate report\n",
        "final_report = generate_final_report(\n",
        "    validation_report, preprocessing_stats, cleanup_results, OUTPUT_DIR\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ðŸ“‹ FINAL REPORT: Phase 3 Complete\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(f\"\\nðŸ“… Completed: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
        "\n",
        "print(f\"\\nðŸ“Š VALIDATION:\")\n",
        "print(f\"   â€¢ Scanned: {final_report['validation']['total_files_scanned']} files\")\n",
        "print(f\"   â€¢ Valid images: {final_report['validation']['valid_images']}\")\n",
        "print(f\"   â€¢ Invalid/Corrupted: {final_report['validation']['invalid_images']}\")\n",
        "\n",
        "print(f\"\\nðŸ”„ PREPROCESSING:\")\n",
        "print(f\"   â€¢ Successfully processed: {final_report['preprocessing']['total_processed']}\")\n",
        "print(f\"   â€¢ Failed: {final_report['preprocessing']['total_failed']}\")\n",
        "print(f\"   â€¢ Output size: {TARGET_SIZE[0]}x{TARGET_SIZE[1]} px\")\n",
        "\n",
        "print(f\"\\nðŸ“‚ OUTPUT DATASET:\")\n",
        "print(f\"   Location: {OUTPUT_DIR}\")\n",
        "print(f\"   Total images: {final_report['output_dataset']['total_images']}\")\n",
        "for class_name, count in sorted(final_report['output_dataset']['classes'].items()):\n",
        "    print(f\"   â””â”€ {class_name}: {count} images\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Report saved to: {OUTPUT_DIR}/preprocessing_report.json\")"
      ],
      "metadata": {
        "id": "generate_report"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Verify Preprocessed Dataset"
      ],
      "metadata": {
        "id": "verify_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def verify_preprocessed_dataset(output_dir: str) -> None:\n",
        "    \"\"\"\n",
        "    Verify the preprocessed dataset meets all requirements.\n",
        "    \"\"\"\n",
        "    print(\"\\nðŸ” VERIFICATION\")\n",
        "    print(\"=\" * 50)\n",
        "    \n",
        "    if not os.path.exists(output_dir):\n",
        "        print(\"âŒ Output directory does not exist!\")\n",
        "        return\n",
        "    \n",
        "    issues = []\n",
        "    sample_stats = []\n",
        "    \n",
        "    for class_name in os.listdir(output_dir):\n",
        "        class_path = os.path.join(output_dir, class_name)\n",
        "        if not os.path.isdir(class_path):\n",
        "            continue\n",
        "        \n",
        "        images = [f for f in os.listdir(class_path) if f.endswith(f'.{IMAGE_FORMAT}')]\n",
        "        \n",
        "        # Check random samples\n",
        "        samples = images[:min(5, len(images))]\n",
        "        for img_name in samples:\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            with Image.open(img_path) as img:\n",
        "                if img.size != TARGET_SIZE:\n",
        "                    issues.append(f\"{class_name}/{img_name}: Wrong size {img.size}\")\n",
        "                if img.mode != COLOR_MODE:\n",
        "                    issues.append(f\"{class_name}/{img_name}: Wrong mode {img.mode}\")\n",
        "                \n",
        "                # Collect stats\n",
        "                arr = np.array(img)\n",
        "                sample_stats.append({\n",
        "                    'class': class_name,\n",
        "                    'mean': arr.mean(),\n",
        "                    'std': arr.std()\n",
        "                })\n",
        "        \n",
        "        print(f\"âœ… {class_name}: {len(images)} images verified\")\n",
        "    \n",
        "    if issues:\n",
        "        print(f\"\\nâš ï¸ Issues found:\")\n",
        "        for issue in issues[:10]:\n",
        "            print(f\"   â€¢ {issue}\")\n",
        "    else:\n",
        "        print(f\"\\nâœ… All images meet specifications!\")\n",
        "        print(f\"   â€¢ Size: {TARGET_SIZE[0]}x{TARGET_SIZE[1]}\")\n",
        "        print(f\"   â€¢ Mode: {COLOR_MODE}\")\n",
        "        print(f\"   â€¢ Format: {IMAGE_FORMAT.upper()}\")\n",
        "    \n",
        "    # Show aggregate stats\n",
        "    if sample_stats:\n",
        "        means = [s['mean'] for s in sample_stats]\n",
        "        stds = [s['std'] for s in sample_stats]\n",
        "        print(f\"\\nðŸ“Š Dataset Statistics (sampled):\")\n",
        "        print(f\"   â€¢ Mean pixel value: {np.mean(means):.2f}\")\n",
        "        print(f\"   â€¢ Std pixel value: {np.mean(stds):.2f}\")\n",
        "\n",
        "\n",
        "# Verify\n",
        "verify_preprocessed_dataset(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "verify_dataset"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Preview Sample Images"
      ],
      "metadata": {
        "id": "preview_header"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "def preview_preprocessed_samples(output_dir: str, samples_per_class: int = 4) -> None:\n",
        "    \"\"\"\n",
        "    Display sample preprocessed images from each class.\n",
        "    \"\"\"\n",
        "    classes = sorted([\n",
        "        d for d in os.listdir(output_dir)\n",
        "        if os.path.isdir(os.path.join(output_dir, d))\n",
        "    ])\n",
        "    \n",
        "    if not classes:\n",
        "        print(\"No classes found!\")\n",
        "        return\n",
        "    \n",
        "    fig, axes = plt.subplots(len(classes), samples_per_class, figsize=(12, 3 * len(classes)))\n",
        "    \n",
        "    if len(classes) == 1:\n",
        "        axes = [axes]\n",
        "    \n",
        "    for i, class_name in enumerate(classes):\n",
        "        class_path = os.path.join(output_dir, class_name)\n",
        "        images = [f for f in os.listdir(class_path) if f.endswith(f'.{IMAGE_FORMAT}')]\n",
        "        \n",
        "        sample_images = random.sample(images, min(samples_per_class, len(images)))\n",
        "        \n",
        "        for j, img_name in enumerate(sample_images):\n",
        "            img_path = os.path.join(class_path, img_name)\n",
        "            img = Image.open(img_path)\n",
        "            \n",
        "            ax = axes[i][j] if len(classes) > 1 else axes[j]\n",
        "            ax.imshow(img)\n",
        "            ax.set_title(f\"{class_name}\\n{TARGET_SIZE[0]}x{TARGET_SIZE[1]}\", fontsize=9)\n",
        "            ax.axis('off')\n",
        "        \n",
        "        for j in range(len(sample_images), samples_per_class):\n",
        "            ax = axes[i][j] if len(classes) > 1 else axes[j]\n",
        "            ax.axis('off')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.suptitle(\"Preprocessed Dataset Samples\", fontsize=14, y=1.02)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Preview\n",
        "preview_preprocessed_samples(OUTPUT_DIR)"
      ],
      "metadata": {
        "id": "preview_samples"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## âœ… Phase 3 Complete!\n",
        "\n",
        "The cleaned and preprocessed dataset is ready at:\n",
        "```\n",
        "/content/drive/MyDrive/DrukFarm/data/preprocessed_potato_dataset/\n",
        "â”œâ”€â”€ Early_Blight/   (224x224 RGB JPG images)\n",
        "â”œâ”€â”€ Late_Blight/    (224x224 RGB JPG images)\n",
        "â””â”€â”€ Healthy/        (224x224 RGB JPG images)\n",
        "```\n",
        "\n",
        "**Dataset Specifications:**\n",
        "- âœ… Image size: 224Ã—224 pixels\n",
        "- âœ… Color format: RGB\n",
        "- âœ… File format: JPEG\n",
        "- âœ… All corrupted images removed/quarantined\n",
        "- âœ… Consistent quality across all images\n",
        "\n",
        "**Next Steps:**\n",
        "- Phase 4: Data Augmentation\n",
        "- Phase 5: Model Training"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}
