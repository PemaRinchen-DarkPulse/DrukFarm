# ðŸŒ¿ Dataset Merger - Complete Documentation Index

## ðŸ“š Quick Navigation

| Document | Purpose | When to Use |
|----------|---------|-------------|
| **[QUICKSTART](DATASET_MERGER_QUICKSTART.md)** | Get started in 5 minutes | First time using the merger |
| **[README](DATASET_MERGER_README.md)** | Full documentation | Understanding features & workflow |
| **[VISUAL OVERVIEW](DATASET_MERGER_VISUAL.md)** | Architecture diagrams | Visual learners, system design |
| **This File** | Navigation hub | Finding specific information |

---

## ðŸš€ Getting Started

### Installation
```bash
# 1. Navigate to ai_services directory
cd ai_services

# 2. Install dependencies
pip install -r requirements.txt

# 3. Verify installation
python validate_merger.py
```

### First Run
```bash
# Run the merger with default settings
python dataset_merger.py

# Or with custom config
python dataset_merger.py custom_config.json
```

---

## ðŸ“– Documentation Files

### 1. DATASET_MERGER_QUICKSTART.md
**5-minute guide to get started**
- Installation steps
- Basic usage
- Configuration
- Output overview
- Common issues

### 2. DATASET_MERGER_README.md
**Comprehensive documentation (80+ pages equivalent)**

**Contents:**
- Overview & Features (10 key features)
- Prerequisites & Dependencies
- Usage & Configuration
- Quality Scoring System
- Complete Workflow
- Output Structure
- Report Explanations
- Deduplication Details
- Clustering Algorithm
- Splitting Strategy
- Customization Guide
- Troubleshooting
- Advanced Usage
- Best Practices
- Future Enhancements

### 3. DATASET_MERGER_VISUAL.md
**Visual system overview with diagrams**

**Contents:**
- System Architecture Diagram
- Data Flow Diagram
- Quality Scoring Breakdown
- Deduplication Strategy Flowchart
- Clustering Visualization
- Split Strategy Flow
- Output Structure Tree
- Usage Examples

---

## ðŸ› ï¸ Core Files

### Python Scripts

| File | Purpose | Run Command |
|------|---------|-------------|
| `dataset_merger.py` | Main merger implementation | `python dataset_merger.py` |
| `validate_merger.py` | Test suite for validation | `python validate_merger.py` |

### Configuration

| File | Purpose | Format |
|------|---------|--------|
| `merger_config.json` | Configuration settings | JSON |
| `requirements.txt` | Python dependencies | Text |

---

## ðŸ“Š Key Components

### 1. DatasetScanner
**Purpose**: Recursively scan and collect images from all datasets

**Features**:
- Supports JPG, PNG, BMP, TIFF
- Auto-detects class labels
- Handles nested directories
- Identifies augmented images

**Location**: `dataset_merger.py` (Lines 65-137)

### 2. ImageAnalyzer
**Purpose**: Extract metadata and compute hashes

**Features**:
- MD5 hashing (exact duplicates)
- Perceptual hashing (similar images)
- Quality scoring (0-100)
- Resolution & file metadata

**Location**: `dataset_merger.py` (Lines 140-224)

### 3. DeduplicationEngine
**Purpose**: Detect and remove duplicates

**Features**:
- MD5-based exact matching
- Perceptual similarity matching
- Quality-based selection
- Complete provenance tracking

**Location**: `dataset_merger.py` (Lines 227-327)

### 4. SimilarityClusterer
**Purpose**: Group similar images using DBSCAN

**Features**:
- Perceptual hash clustering
- Hamming distance calculation
- DBSCAN algorithm
- Prevents data leakage

**Location**: `dataset_merger.py` (Lines 330-380)

### 5. DatasetSplitter
**Purpose**: Create stratified train/val/test/holdout splits

**Features**:
- Holdout set (10%)
- Stratified by class
- Cluster-aware splitting
- Source diversity

**Location**: `dataset_merger.py` (Lines 383-495)

### 6. DatasetMerger
**Purpose**: Orchestrate entire merging process

**Features**:
- Pipeline coordination
- File organization
- Report generation
- Summary creation

**Location**: `dataset_merger.py` (Lines 498-730)

---

## ðŸ“ Output Files

### Merged Dataset Structure
```
merged_output/
â””â”€â”€ merged_dataset/
    â”œâ”€â”€ train/          # 70% of remaining
    â”œâ”€â”€ val/            # 15% of remaining
    â”œâ”€â”€ test/           # 15% of remaining
    â””â”€â”€ holdout/        # 10% of total
```

### Reports Generated

| File | Description | Use Case |
|------|-------------|----------|
| `merged_metadata.csv` | Complete image metadata | Data analysis, filtering |
| `provenance_map.csv` | Full provenance tracking | Audit trail, traceability |
| `duplicates_report.csv` | Removed duplicates list | Verification, debugging |
| `similarity_clusters.csv` | Clustering results | Understanding groupings |
| `split_files_list.csv` | Split assignments | Custom data loading |
| `MERGE_SUMMARY.md` | Human-readable summary | Overview, statistics |

---

## ðŸŽ¯ Common Tasks

### Task: View Merged Dataset Statistics
```bash
# After running merger, read:
cat merged_output/reports/MERGE_SUMMARY.md
```

### Task: Filter High-Quality Images
```python
import pandas as pd
df = pd.read_csv('merged_output/reports/merged_metadata.csv')
high_quality = df[df['quality_score'] > 75]
print(f"Found {len(high_quality)} high-quality images")
```

### Task: Check Removed Duplicates
```python
import pandas as pd
dups = pd.read_csv('merged_output/reports/duplicates_report.csv')
print(f"Removed {len(dups)} duplicates")
print(dups.groupby('reason').size())
```

### Task: Analyze Class Distribution
```python
import pandas as pd
splits = pd.read_csv('merged_output/reports/split_files_list.csv')
for split in ['train', 'val', 'test', 'holdout']:
    split_data = splits[splits['split'] == split]
    print(f"\n{split.upper()}:")
    print(split_data['canonical_class'].value_counts())
```

### Task: Load Data in PyTorch
```python
from torch.utils.data import Dataset
import pandas as pd
from PIL import Image

class MergedPlantDataset(Dataset):
    def __init__(self, split='train', transform=None):
        splits_df = pd.read_csv('merged_output/reports/split_files_list.csv')
        self.data = splits_df[splits_df['split'] == split]
        self.transform = transform
        
        # Create class to index mapping
        self.classes = sorted(self.data['canonical_class'].unique())
        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        img_path = row['image_path']
        label = self.class_to_idx[row['canonical_class']]
        
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)
        
        return image, label

# Usage
train_dataset = MergedPlantDataset('train', transform=your_transform)
```

---

## âš™ï¸ Configuration Guide

### Basic Configuration
Edit `merger_config.json`:

```json
{
  "merger_config": {
    "paths": {
      "datasets_root": "path/to/myDatasets",
      "output_root": "path/to/output"
    },
    "splitting": {
      "train_ratio": 0.7,
      "val_ratio": 0.15,
      "test_ratio": 0.15,
      "holdout_ratio": 0.1
    }
  }
}
```

### Advanced Configuration

**More Aggressive Deduplication**:
```json
"deduplication": {
  "perceptual_threshold": 10  // Find more similar images
}
```

**Tighter Clustering**:
```json
"clustering": {
  "eps": 0.2,              // Smaller clusters
  "min_samples": 3         // Stricter grouping
}
```

**Custom Quality Weights**:
```json
"quality_scoring": {
  "resolution_weight": 50,   // Prioritize resolution
  "sharpness_weight": 40,    // Emphasize sharpness
  "aspect_ratio_weight": 5,
  "file_size_weight": 5
}
```

---

## ðŸ› Troubleshooting

### Problem: Validation Fails
**Solution**:
```bash
python validate_merger.py
# Check error messages
# Verify dependencies: pip install -r requirements.txt
```

### Problem: Out of Memory
**Solutions**:
1. Process fewer images at once
2. Reduce image resolution
3. Add more RAM
4. Process datasets individually

### Problem: Slow Performance
**Solutions**:
```python
# In dataset_merger.py, disable perceptual hashing:
# Comment out in ImageAnalyzer.analyze_image():
# p_hash = str(imagehash.phash(img))
```

### Problem: Too Many Duplicates Removed
**Solutions**:
```json
// In merger_config.json, reduce threshold:
"deduplication": {
  "perceptual_threshold": 3  // More conservative
}
```

### Problem: Imbalanced Splits
**Check original distribution**:
```python
import pandas as pd
metadata = pd.read_csv('merged_output/reports/merged_metadata.csv')
print(metadata['canonical_class'].value_counts())
```

---

## ðŸ“ˆ Performance Benchmarks

Expected performance on typical hardware:

| Dataset Size | Scanning | Analysis | Deduplication | Clustering | Total Time |
|--------------|----------|----------|---------------|------------|------------|
| 1,000 images | ~10s | ~30s | ~5s | ~10s | ~1 min |
| 5,000 images | ~30s | ~2m | ~15s | ~30s | ~3 min |
| 10,000 images | ~1m | ~5m | ~30s | ~1m | ~8 min |
| 50,000 images | ~5m | ~25m | ~2m | ~5m | ~40 min |

*Benchmarks on: Intel i7, 16GB RAM, SSD storage*

---

## ðŸ” Best Practices

### 1. Data Safety
- âœ… Never modify original datasets
- âœ… Backup before running
- âœ… Version control configurations
- âœ… Keep provenance records

### 2. Quality Control
- âœ… Run validation first
- âœ… Review duplicates report
- âœ… Check class distributions
- âœ… Manually inspect samples

### 3. Splitting Strategy
- âœ… Keep holdout untouched
- âœ… Verify no data leakage
- âœ… Ensure class balance
- âœ… Test on holdout last

### 4. Documentation
- âœ… Save merger configuration
- âœ… Keep merge summary
- âœ… Document modifications
- âœ… Track dataset versions

---

## ðŸ”¬ Research Applications

### Use Case 1: Transfer Learning
```python
# Use high-quality images for pre-training
df = pd.read_csv('merged_output/reports/merged_metadata.csv')
splits = pd.read_csv('merged_output/reports/split_files_list.csv')

# Select high-quality train images
train_df = splits[splits['split'] == 'train']
quality_df = df[df['quality_score'] > 80]
pretrain_data = train_df.merge(quality_df, on='image_path')
```

### Use Case 2: Domain Generalization
```python
# Holdout set has diverse sources - perfect for testing
holdout = splits[splits['split'] == 'holdout']
print("Holdout dataset sources:")
print(holdout['dataset_source'].value_counts())
```

### Use Case 3: Data Augmentation Planning
```python
# Identify classes with few samples
class_counts = splits[splits['split'] == 'train'].groupby('canonical_class').size()
small_classes = class_counts[class_counts < 100]
print("Classes needing augmentation:", small_classes)
```

---

## ðŸ“ž Support & Resources

### Getting Help
1. Check this index for navigation
2. Review relevant documentation section
3. Run validation script: `python validate_merger.py`
4. Check generated reports in `merged_output/reports/`

### Additional Resources
- **Full README**: `DATASET_MERGER_README.md`
- **Quick Start**: `DATASET_MERGER_QUICKSTART.md`
- **Visual Guide**: `DATASET_MERGER_VISUAL.md`
- **Source Code**: `dataset_merger.py`
- **Validation**: `validate_merger.py`

---

## ðŸ“ Version History

**Version 1.0.0** (November 2025)
- Initial release
- Core features: scanning, deduplication, clustering, splitting
- Comprehensive documentation
- Validation suite
- Configuration support

---

## ðŸŽ¯ Quick Reference

### Essential Commands
```bash
# Install dependencies
pip install -r requirements.txt

# Validate setup
python validate_merger.py

# Run merger (default config)
python dataset_merger.py

# Run merger (custom config)
python dataset_merger.py my_config.json
```

### Key Paths
```
Input:  ai_services/myDatasets/
Output: ai_services/merged_output/
Config: ai_services/merger_config.json
Docs:   ai_services/DATASET_MERGER_*.md
```

### Important Ratios (Default)
- Train: 70% (of remaining after holdout)
- Validation: 15%
- Test: 15%
- Holdout: 10% (of total)

### Quality Score Components
- Resolution: 40 points
- Sharpness: 30 points
- File Size: 20 points
- Aspect Ratio: 10 points

---

**Last Updated**: November 15, 2025  
**Author**: Dataset Integration Engineer  
**Project**: DrukFarm AI Services
