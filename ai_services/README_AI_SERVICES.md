# ğŸŒ¿ DrukFarm AI Services - Dataset Management Suite

> **Complete toolkit for managing, merging, analyzing, and preparing plant disease datasets for deep learning**

---

## ğŸ¯ What You Get

A production-ready suite of tools for dataset management:

1. **ğŸ”„ Dataset Merger** - Merge multiple datasets with deduplication
2. **ğŸ“Š Dataset Curator** - Analyze dataset quality and characteristics  
3. **ğŸ·ï¸ Taxonomy Designer** - Create standardized label systems
4. **ğŸ“ˆ Dataset Visualizer** - Generate insights and visualizations
5. **ğŸ§¹ Dataset Cleaner** - Interactive data cleaning

---

## âš¡ Quick Start (3 Commands)

```bash
# 1. Activate environment
cd ai_services
drukFarmVenv\Scripts\activate

# 2. Install dependencies
pip install -r requirements.txt

# 3. Merge all datasets
python dataset_merger.py
```

**Done!** Your merged dataset is in `merged_output/`

---

## ğŸ“š What's Inside

```
ai_services/
â”‚
â”œâ”€â”€ ğŸ”„ DATASET MERGER (NEW!)
â”‚   â”œâ”€â”€ dataset_merger.py              # Main merger
â”‚   â”œâ”€â”€ validate_merger.py             # Validation suite
â”‚   â”œâ”€â”€ merger_config.json             # Configuration
â”‚   â””â”€â”€ requirements.txt               # Dependencies
â”‚
â”œâ”€â”€ ğŸ“Š DATASET CURATOR
â”‚   â”œâ”€â”€ dataset_curator.py             # Analysis engine
â”‚   â”œâ”€â”€ dataset_visualizer.py          # Visualizations
â”‚   â”œâ”€â”€ dataset_cleaner.py             # Interactive cleaning
â”‚   â”œâ”€â”€ curator_config.json            # Configuration
â”‚   â””â”€â”€ (uses requirements.txt)        # Dependencies
â”‚
â”œâ”€â”€ ğŸ·ï¸ TAXONOMY DESIGNER
â”‚   â”œâ”€â”€ label_taxonomy_designer.py     # Taxonomy creation
â”‚   â””â”€â”€ dataset_reorganizer.py         # Apply taxonomy
â”‚
â”œâ”€â”€ ğŸ“ DATA
â”‚   â”œâ”€â”€ myDatasets/                    # Input datasets
â”‚   â”œâ”€â”€ merged_output/                 # Merger output
â”‚   â”œâ”€â”€ dataset_analysis_output/       # Curator output
â”‚   â””â”€â”€ taxonomy_output/               # Taxonomy output
â”‚
â””â”€â”€ ğŸ“– DOCUMENTATION
    â”œâ”€â”€ DATASET_MERGER_SUMMARY.md      # Merger overview â­
    â”œâ”€â”€ DATASET_MERGER_README.md       # Merger full docs
    â”œâ”€â”€ DATASET_MERGER_INDEX.md        # Merger navigation
    â”œâ”€â”€ IMPLEMENTATION_SUMMARY.md      # Curator overview â­
    â”œâ”€â”€ TAXONOMY_IMPLEMENTATION_SUMMARY.md  # Taxonomy overview â­
    â”œâ”€â”€ COMPLETE_WORKFLOW.md           # End-to-end guide â­
    â””â”€â”€ INDEX.md                       # Main navigation â­
```

---

## ğŸš€ Recommended Workflow

### Step 1: Merge Datasets (Start Here!)

**What it does**: Combines all datasets, removes duplicates, creates splits

```bash
pip install -r requirements.txt
python validate_merger.py
python dataset_merger.py
```

**Output**: 
- Unified dataset in `merged_output/merged_dataset/`
- Splits: train (70%), val (15%), test (15%), holdout (10%)
- Reports in `merged_output/reports/`

**Documentation**: `DATASET_MERGER_SUMMARY.md`

---

### Step 2: Analyze Quality (Recommended)

**What it does**: Analyzes merged dataset for quality and characteristics

```bash
pip install -r requirements.txt
python dataset_curator.py
python dataset_visualizer.py
```

**Output**:
- Statistics and quality reports
- Visualizations and charts
- Sample images for review

**Documentation**: `IMPLEMENTATION_SUMMARY.md`

---

### Step 3: Design Taxonomy (Optional)

**What it does**: Creates standardized label system

```bash
python label_taxonomy_designer.py
```

**Output**:
- Canonical label taxonomy
- Label mapping rules
- Migration plan

**Documentation**: `TAXONOMY_IMPLEMENTATION_SUMMARY.md`

---

### Step 4: Train Your Model

Use the merged dataset:

```python
from torch.utils.data import DataLoader
from torchvision import transforms

# Your training code here
train_dir = 'merged_output/merged_dataset/train/'
val_dir = 'merged_output/merged_dataset/val/'
test_dir = 'merged_output/merged_dataset/test/'
holdout_dir = 'merged_output/merged_dataset/holdout/'
```

---

## ğŸ“ Documentation Guide

**New to the system?** Read these in order:

### ğŸ”„ Start with Merger
1. **DATASET_MERGER_SUMMARY.md** - 5-minute overview
2. **DATASET_MERGER_QUICKSTART.md** - Quick setup
3. **DATASET_MERGER_README.md** - Full documentation

### ğŸ“Š Then Analyze
1. **IMPLEMENTATION_SUMMARY.md** - Curator overview
2. **QUICK_REFERENCE.md** - Common tasks
3. **COMPLETE_GUIDE.md** - Full walkthrough

### ğŸ·ï¸ Design Taxonomy
1. **TAXONOMY_IMPLEMENTATION_SUMMARY.md** - Taxonomy overview
2. **TAXONOMY_DESIGNER_GUIDE.md** - Complete guide

### ğŸ¯ See Complete Picture
1. **COMPLETE_WORKFLOW.md** - End-to-end workflow
2. **INDEX.md** - Master navigation

---

## ğŸ“Š Features Comparison

| Feature | Merger | Curator | Taxonomy |
|---------|--------|---------|----------|
| **Combine datasets** | âœ… | âŒ | âŒ |
| **Remove duplicates** | âœ… | âš ï¸ Detect only | âŒ |
| **Quality scoring** | âœ… | âœ… | âŒ |
| **Create splits** | âœ… | âŒ | âŒ |
| **Provenance tracking** | âœ… | âŒ | âŒ |
| **Statistics** | âš ï¸ Basic | âœ… Detailed | âŒ |
| **Visualizations** | âŒ | âœ… | âš ï¸ Basic |
| **Label standardization** | âŒ | âŒ | âœ… |
| **Interactive cleaning** | âŒ | âœ… | âŒ |

**Recommended**: Use all three for best results!

---

## âš™ï¸ System Requirements

### Required
- Python 3.8+
- Windows/Linux/Mac
- 8GB RAM (minimum)
- 10GB free disk space

### Python Packages
```bash
# Merger
numpy, pandas, pillow, opencv-python, scikit-learn, imagehash

# Curator  
numpy, pandas, pillow, matplotlib, seaborn

# All included in requirements_*.txt files
```

---

## ğŸ“ˆ Performance

Expected processing times:

| Dataset Size | Merge | Analyze | Taxonomy | Total |
|--------------|-------|---------|----------|-------|
| 1K images | 1 min | 2 min | 1 min | ~5 min |
| 10K images | 8 min | 15 min | 5 min | ~30 min |
| 50K images | 40 min | 60 min | 15 min | ~2 hrs |

*Intel i7, 16GB RAM, SSD*

---

## ğŸ¯ Key Features

### Dataset Merger
- âœ… MD5 + perceptual hashing deduplication
- âœ… Quality scoring (0-100)
- âœ… Complete provenance tracking
- âœ… Similarity clustering
- âœ… Stratified splits with holdout
- âœ… Organized directory structure
- âœ… Comprehensive CSV reports

### Dataset Curator
- âœ… Manifest generation
- âœ… Statistical analysis
- âœ… Duplicate detection
- âœ… Label analysis
- âœ… Quality assessment
- âœ… Sample extraction
- âœ… Visualizations
- âœ… Health reports

### Taxonomy Designer
- âœ… Synonym detection
- âœ… Canonical naming
- âœ… Mapping rules
- âœ… Migration planning
- âœ… Visualization
- âœ… Export formats

---

## ğŸ› Troubleshooting

### Installation Issues
```bash
# Try upgrading pip
python -m pip install --upgrade pip

# Install packages individually
pip install numpy pandas pillow opencv-python scikit-learn imagehash
```

### Memory Issues
- Process fewer images at once
- Reduce image resolution
- Use 64-bit Python
- Add more RAM

### Slow Performance
- Use SSD storage
- Close other programs
- Process datasets individually
- Skip perceptual hashing

### See Full Troubleshooting
- Merger: `DATASET_MERGER_README.md` â†’ Troubleshooting section
- Curator: `COMPLETE_GUIDE.md` â†’ Troubleshooting section

---

## ğŸ“– Example Usage

### Example 1: Quick Merge
```bash
python dataset_merger.py
cat merged_output\reports\MERGE_SUMMARY.md
```

### Example 2: Full Pipeline
```bash
# Merge
python dataset_merger.py

# Analyze
python dataset_curator.py
python dataset_visualizer.py

# Taxonomy
python label_taxonomy_designer.py
```

### Example 3: Load in PyTorch
```python
import pandas as pd
from torch.utils.data import Dataset, DataLoader
from PIL import Image

class MergedDataset(Dataset):
    def __init__(self, split='train'):
        splits = pd.read_csv('merged_output/reports/split_files_list.csv')
        self.data = splits[splits['split'] == split]
        self.classes = sorted(self.data['canonical_class'].unique())
        self.class_to_idx = {c: i for i, c in enumerate(self.classes)}
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        row = self.data.iloc[idx]
        image = Image.open(row['image_path']).convert('RGB')
        label = self.class_to_idx[row['canonical_class']]
        return image, label

# Use it
train_dataset = MergedDataset('train')
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
```

---

## ğŸ” What Gets Generated

### After Merger
```
merged_output/
â”œâ”€â”€ merged_dataset/
â”‚   â”œâ”€â”€ train/           # Training images
â”‚   â”œâ”€â”€ val/             # Validation images
â”‚   â”œâ”€â”€ test/            # Test images
â”‚   â””â”€â”€ holdout/         # Final evaluation images
â””â”€â”€ reports/
    â”œâ”€â”€ merged_metadata.csv        # All metadata
    â”œâ”€â”€ provenance_map.csv         # Provenance tracking
    â”œâ”€â”€ duplicates_report.csv      # Removed duplicates
    â”œâ”€â”€ similarity_clusters.csv    # Image clusters
    â”œâ”€â”€ split_files_list.csv       # Split assignments
    â””â”€â”€ MERGE_SUMMARY.md           # Summary report
```

### After Curator
```
dataset_analysis_output/
â”œâ”€â”€ manifests/
â”œâ”€â”€ statistics/
â”œâ”€â”€ duplicates/
â”œâ”€â”€ labels/
â”œâ”€â”€ quality/
â”œâ”€â”€ samples/
â”œâ”€â”€ visualizations/
â””â”€â”€ health_report.md
```

### After Taxonomy
```
taxonomy_output/
â”œâ”€â”€ canonical_taxonomy.json
â”œâ”€â”€ label_mappings.csv
â”œâ”€â”€ taxonomy_visualization.html
â””â”€â”€ migration_plan.md
```

---

## ğŸ“ Best Practices

1. **Always backup** original datasets
2. **Run validation** before full merge
3. **Review reports** after processing
4. **Keep holdout** untouched until final evaluation
5. **Document** your configuration
6. **Version control** your configs and scripts

---

## ğŸ“ Getting Help

### Quick Help
1. Check documentation in this folder
2. Run validation: `python validate_merger.py`
3. Review generated reports

### Documentation Index
- **Navigation**: `INDEX.md`
- **Workflow**: `COMPLETE_WORKFLOW.md`
- **Merger**: `DATASET_MERGER_SUMMARY.md`
- **Curator**: `IMPLEMENTATION_SUMMARY.md`
- **Taxonomy**: `TAXONOMY_IMPLEMENTATION_SUMMARY.md`

---

## ğŸ“œ Version History

**Version 2.0.0** (November 2025)
- âœ¨ Added Dataset Merger
- âœ¨ Comprehensive documentation
- âœ¨ Validation suite
- âœ¨ Complete workflow guide

**Version 1.0.0** (Previous)
- Dataset Curator
- Taxonomy Designer
- Basic documentation

---

## ğŸ¯ Next Steps

1. **Review** this README
2. **Read** `COMPLETE_WORKFLOW.md` for full workflow
3. **Install** dependencies: `pip install -r requirements.txt`
4. **Validate** setup: `python validate_merger.py`
5. **Run** merger: `python dataset_merger.py`
6. **Check** results: `cat merged_output\reports\MERGE_SUMMARY.md`
7. **Analyze** (optional): `python dataset_curator.py`
8. **Train** your model!

---

## ğŸ“„ License

Part of the DrukFarm project

---

## ğŸ™ Acknowledgments

Tools created for the DrukFarm agricultural AI platform, designed to help farmers identify plant diseases through mobile applications.

---

**Ready to get started?**

```bash
cd ai_services
drukFarmVenv\Scripts\activate
pip install -r requirements.txt
python dataset_merger.py
```

**Happy dataset merging! ğŸŒ¿**
